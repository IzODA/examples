{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Retention Demo\n",
    "Simple demo to show Anaconda functionality on the mainframe accessing mainframe data with the Optimized Data Layer client, dsdbc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use dsdbc, if you have the necessary files virtualized into ODL, simply comment out the next line\n",
    "csv = \"yes\"\n",
    "\n",
    "if not csv:\n",
    "    import dsdbc #This package required to interface with ODL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Remove font warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Mainframe Data Connections\n",
    "This step will setup the Optimized Data Layer client connection to access mainframe data and load them into Panadas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not csv:\n",
    "    conn = dsdbc.connect()\n",
    "    cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Client Data***\n",
    "\n",
    "Load client data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pandas dataframe directly from \"DB\" query (really a pysical sequential dataset)\n",
    "if csv:\n",
    "    client_df = pd.read_csv(\"data/CLIENT_INFO_VSAMKSDS.csv\")\n",
    "else:\n",
    "    client_df = pd.read_sql('SELECT * FROM CLIENT_INFO_VSAMKSDS', conn)\n",
    "client_df = client_df.set_index(\"CONT_ID\")\n",
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Credit transactions***\n",
    "\n",
    "Load credit card transactions into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv:\n",
    "    txn_df = pd.read_csv(\"data/SPPAYTB_VSAM-1.csv\")\n",
    "    txn_df2 = pd.read_csv(\"data/SPPAYTB_VSAM-2.csv\")\n",
    "    txn_df = txn_df.append(txn_df2)\n",
    "    txn_df3 = pd.read_csv(\"data/SPPAYTB_VSAM-3.csv\")\n",
    "    txn_df = txn_df.append(txn_df3)\n",
    "else:\n",
    "    txn_df = pd.read_sql('SELECT * FROM SPPAYTB_VSAM', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df['AUREQ_TX_DT_TTLAMT'] = pd.to_numeric(txn_df['AUREQ_TX_DT_TTLAMT'])\n",
    "txn_df['CONT_ID'] = txn_df['CONT_ID'].astype('int64')\n",
    "txn_df['HDR_CREDTT'] = pd.to_datetime(txn_df['HDR_CREDTT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df['DATE'] = txn_df['HDR_CREDTT'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "txn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate statistics\n",
    "Calculate a few aggregate statistics based on credit transactions and join the results to the client data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total txns per customer\n",
    "total_txns_df = txn_df.groupby('CONT_ID').size().rename(\"TOTAL_TXNS\").to_frame()\n",
    "client_df = client_df.join(total_txns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total transaction amounts per customer\n",
    "total_txn_amount_df = txn_df.groupby('CONT_ID')['AUREQ_TX_DT_TTLAMT'].sum().rename(\"TOTAL_TXN_AMOUNT\").to_frame()\n",
    "client_df = client_df.join(total_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg transaction amounts per customer\n",
    "avg_txn_amount_df = txn_df.groupby('CONT_ID')['AUREQ_TX_DT_TTLAMT'].mean().rename(\"AVG_TXN_AMOUNT\").to_frame()\n",
    "client_df = client_df.join(avg_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily txns per customer\n",
    "daily_txns = txn_df.groupby(['DATE', 'CONT_ID']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing txns on a particular day means customer had none.\n",
    "# These days should be included in the avg as 0 transaction days.\n",
    "avg_daily_txns_df = daily_txns.unstack().fillna(0).mean().rename(\"AVG_DAILY_TXNS\").to_frame()\n",
    "client_df = client_df.join(avg_daily_txns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: importing pymon packages will fail, because they are not currently installed.  To install them, run the following command***\n",
    "\n",
    "```conda install matplotlib```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot X vs. Y\n",
    "We begin our exploration of the data set by creating some scatterplots of each column vs. the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointplot(x, y, data, **kwargs):\n",
    "    size = kwargs.pop('size', 10)\n",
    "    alpha = kwargs.pop('alpha', 0.3)\n",
    "    return sns.jointplot(x=x, y=y, data=data, \n",
    "                         alpha=alpha,\n",
    "                         size=size,\n",
    "                         **kwargs)\n",
    "\n",
    "# for widget\n",
    "def w_jointplot(x, y):\n",
    "    g = jointplot(x, y, filter_outliers(client_df, by_col=y))\n",
    "    plt.close()\n",
    "    return g.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_labels = ['Did not churn', 'Did churn']\n",
    "\n",
    "def filter_outliers(d, by_col=None):\n",
    "    if isinstance(d, pd.\n",
    "                  Series):\n",
    "        return d[((d-d.mean()).abs()<=3*d.std())]\n",
    "    elif isinstance(d, pd.DataFrame):\n",
    "        if not by_col:\n",
    "            raise ValueError('by_col is required for DataFrame')\n",
    "        return d[np.abs(d[by_col]-d[by_col].mean())<=(3*d[by_col].std())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = jointplot('AGE_YEARS', 'ANNUAL_INCOME', filter_outliers(client_df, by_col='ANNUAL_INCOME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "Next, we compute the correlation coefficients between each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = client_df.corr()\n",
    "\n",
    "# only show lower triangle\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(corr, mask=mask, square=True, annot=True, fmt='.2f',\n",
    "                 cbar=True,\n",
    "                 ax=ax)\n",
    "title = ax.set_title('Correlations', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn\n",
    "We plot the distributions of clients who churned and those that did not on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn_by(df, col, **kwargs):\n",
    "    f, ax = plt.subplots(figsize=(12,10), sharex=True)\n",
    "    kde = kwargs.get('kde', False)\n",
    "    hist = kwargs.get('hist', False)\n",
    "    for churn in df.CHURN.unique():\n",
    "        sns.distplot(df[df.CHURN == churn][col], \n",
    "                     label=churn_labels[churn], \n",
    "                     kde_kws={'shade': (kde and not hist)},\n",
    "                     ax=ax, \n",
    "                     **kwargs)\n",
    "\n",
    "    ax.set_title('Client Churn by {}'.format(col))\n",
    "    label = ax.set_xlabel('{}'.format(col))\n",
    "    return f, ax\n",
    "\n",
    "def w_plot_churn_by(column, hist=True, kde=False, norm_hist=False):\n",
    "    df = filter_outliers(client_df, by_col=column)\n",
    "    f, ax = plot_churn_by(df, column, hist=hist, kde=kde, norm_hist=norm_hist)\n",
    "    plt.legend()\n",
    "    plt.close()\n",
    "    return f\n",
    "\n",
    "f, ax = plot_churn_by(client_df, 'AGE_YEARS')\n",
    "ax = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features that showed a negative correlation with churn were age and activity level. Here we generate a boxplot with those two features as the axes, and churn as the category.\n",
    "The plot shows that clients that churn tend to be younger across all levels of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'AGE_YEARS'\n",
    "data = filter_outliers(client_df, by_col=col)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(x='ACTIVITY_LEVEL', y=col, hue=\"CHURN\", data=data, \n",
    "                 palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This beeswarm plot shows clients binned by the level of activity they maintain with the bank. Clients that churned maintained lower levels of activity (0-2). And of clients within these lower activity levels, younger clients churned more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.swarmplot(x='ACTIVITY_LEVEL', y='AGE_YEARS', hue='CHURN', \n",
    "                   data=data.sample(n=2000, random_state=51), \n",
    "                   palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train churn model\n",
    "Train a churn classifier, which we'll use to predict the probability that a client will churn.\n",
    "To keep things simple, we use a single data set, which we split into training and test data sets. We use the training data to train the model, and the test data to make projections about lost revenue to the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_space(df):\n",
    "    '''Create the feature space required by our classifier.'''\n",
    "    # drop columns/features we don't want/need for the classifier\n",
    "    features_df = df.drop(['CHURN', 'CUSTOMER_ID'], axis=1, errors='ignore')\n",
    "    X = features_df.as_matrix().astype(np.float)\n",
    "    # normalize feature values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def predict_churn(X):\n",
    "    '''Predict the probabilit of churn from feature set.'''\n",
    "    return clf.predict_proba(X)[:,1]\n",
    "\n",
    "def train_model(X, y):\n",
    "    '''Train our classifier using features X and target variable y.'''\n",
    "    clf = RF(n_estimators=100)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "def init_model(df):\n",
    "    # split data into train, test sets\n",
    "    train_index, test_index = train_test_split(df.index, random_state=99)\n",
    "    train_df = client_df.ix[train_index]\n",
    "    test_df = client_df.ix[test_index]\n",
    "\n",
    "    # target variable\n",
    "    y = np.array(train_df['CHURN'])\n",
    "\n",
    "    # extract features\n",
    "    X = make_feature_space(train_df)\n",
    "\n",
    "    # train classifier\n",
    "    clf = train_model(X, y)\n",
    "\n",
    "    return clf, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we are left with the churn classifier and the test data set, which we'll use for our churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, test_df = init_model(client_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate business loss\n",
    "In this simple example, we calculate the projected loss of business (revenue) to BigBank for all clients in the test data set. We calculate BigBank's revenue from each client, and multiply that by the churn probability to determine the predicted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_business_loss(df):\n",
    "    #df['customer_id'] = df.index\n",
    "    data = df.copy()\n",
    "\n",
    "    # extract features\n",
    "    X = make_feature_space(df)\n",
    "    \n",
    "    # predict churn\n",
    "    data['churn_probability'] = predict_churn(X)\n",
    "    \n",
    "    # TODO: avg_daily_balance would be a nice feature to have here\n",
    "    # for now, we'll just use fraction of income\n",
    "    avg_daily_balance = df['ANNUAL_INCOME'] / 6\n",
    "\n",
    "    # Interest made on deposits\n",
    "    deposit_rate = 0.02\n",
    "\n",
    "    # Fee collected for each credit txn\n",
    "    credit_rate = 0.015\n",
    "\n",
    "    # Assume we make some money on trading fees and/or portfolio management\n",
    "    mgmt_rate = 0.02\n",
    "\n",
    "    # How much is each customer worth to the business?\n",
    "    worth = deposit_rate * avg_daily_balance + \\\n",
    "            mgmt_rate * df['ANNUAL_INVEST'] + \\\n",
    "            credit_rate * df['TOTAL_TXN_AMOUNT']\n",
    "    data['worth'] = worth\n",
    "    \n",
    "    # How much would we lose per annum?\n",
    "    data['predicted_loss'] = data['churn_probability'] * worth\n",
    "    \n",
    "    return data.sort_values(by='predicted_loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = calc_business_loss(test_df)\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss by Age Group\n",
    "In this section, we calculate and plot the projected loss of revenue by age group. In our data set, age is an important feature in predicting if a client will churn.\n",
    "First we create a DataFrame containing the cumulative predicted loss by age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_age(df, bins=None):\n",
    "    if bins is None:\n",
    "        bin_size = 5\n",
    "        _min, _max = int(df.AGE_YEARS.min()), int(df.AGE_YEARS.max())\n",
    "        bins = range(_min, _max + bin_size, 5)\n",
    "    return df.groupby(pd.cut(df.AGE_YEARS, bins=bins))\n",
    "\n",
    "data_by_age = churn_df.pipe(group_by_age)\n",
    "data_by_age['predicted_loss'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_by_age_df = data_by_age['predicted_loss'].sum().reset_index()\n",
    "loss_by_age_df['AGE_YEARS'] = loss_by_age_df['AGE_YEARS'].astype(str)\n",
    "\n",
    "loss_by_age_df.plot(x='AGE_YEARS', y='predicted_loss', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
